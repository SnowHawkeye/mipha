{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## A - Data extraction\n",
    "### Input\n",
    "Raw data from a database.\n",
    "\n",
    "### Output\n",
    "For each data source, a matrix containing the data.\n",
    "For a first version we will include:\n",
    "- Demographics (age and gender)\n",
    "- Laboratory analyses (~20 most common identified in previous work)\n",
    "- Another source (either ECG, medication, other diagnoses, etc. -- ECG might be the most straightforward because it is numerical data)\n",
    "\n",
    "### Notes\n",
    "Options for the output matrix size:\n",
    "- *Option 1*: align all matrices on the same time frame with the same resolution (per month, per year, etc.).\n",
    "- *Option 2*: treat each matrix individually with their own time frame and resolution (each data source is parametrized individually).\n",
    "\n",
    "We may want to treat all laboratory analyses as one to take into account their combined variations. This is also a strong argument to go for option 1 (when the data type allows for it).\n",
    "\n",
    "## B - Feature extraction\n",
    "### Input\n",
    "Individual matrices from each data source.\n",
    "\n",
    "### Output\n",
    "New matrices containing extracted features.\n",
    "- *Option 1*: the output has a user-defined size, most likely using ML techniques\n",
    "- *Option 2*: the output has a pre-determined size because there is a set amount of \"manually-extracted\" features\n",
    "- *Option 3*: we have a mix of options 1 and 2 because we want to leave the possibility to design the feature extractor (ML methods should not be forced)\n",
    "\n",
    "### Notes\n",
    "Option 3 seems to be the most flexible. Standardizing the size of matrices should probably be the role of the aggregator.\n",
    "For now, we could implement manual extraction and ML extraction for each data source.\n",
    "\n",
    "**Should the feature extractor use its own backpropagation, or backpropagation from the whole system?**\n",
    "\n",
    "## C - Feature aggregation\n",
    "### Input\n",
    "Output matrices from the feature extractors.\n",
    "\n",
    "### Output\n",
    "A single matrix usable for machine learning, containing aggregated features for each example.\n",
    "\n",
    "### Notes\n",
    "A form of weighted aggregation could be interesting to implement.\n",
    "\n",
    "## D - Cleaning\n",
    "\n",
    "## E - Machine Learning\n",
    "\n",
    "## F - Evaluation"
   ],
   "id": "5466239c69804c97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tsfel  # time series feature extraction"
   ],
   "id": "91d13737bed99efa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "DIAGNOSIS_CODE = \"icd_code\"\n",
    "ADMISSION_ID = \"hadm_id\"\n",
    "PATIENT_ID = \"subject_id\"\n",
    "ANALYSIS_ID = \"itemid\"\n",
    "SPECIMEN_ID = \"specimen_id\"\n",
    "ADMISSION_TIME = \"admittime\"\n",
    "AGE = \"anchor_age\"\n",
    "DIAGNOSIS_TIME = \"diagnosis_time\"\n",
    "ANALYSIS_TIME = \"storetime\"\n",
    "ANALYSIS_NAME = \"label\"\n",
    "ANALYSIS_VALUE = \"valuenum\"\n",
    "DATA_PATH = \"C:/Biologie/mimic_data/matrices/\""
   ],
   "id": "78dd695a7da1e627",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def table_path(table_name):\n",
    "    return f\"C:/mimic_data/mimic_iv/mimic-iv-2.2/hosp/{table_name}.csv.gz\"\n",
    "\n",
    "\n",
    "diagnoses_lookup = pd.read_csv(table_path(\"d_icd_diagnoses\"))\n",
    "analyses_lookup = pd.read_csv(table_path(\"d_labitems\"))\n",
    "patients = pd.read_csv(table_path(\"patients\"))\n",
    "import pickle\n",
    "\n",
    "STORAGE = \"stage_5_ckd/\"\n",
    "\n",
    "with open(DATA_PATH + STORAGE + 'positive_patients_matrices.pkl', 'rb') as handle: x_pos, y_pos, meta_pos = pickle.load(\n",
    "    handle)\n",
    "with open(DATA_PATH + STORAGE + 'negative_patients_matrices.pkl', 'rb') as handle: x_neg, y_neg, meta_neg = pickle.load(\n",
    "    handle)"
   ],
   "id": "112074038c6818d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def split_data(x, y, meta, test_size=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Split data between train and test set based on the patients' IDs.\n",
    "    :return: x_train, x_test, y_train, y_test, meta_train, meta_test\n",
    "    \"\"\"\n",
    "    df_id = pd.DataFrame([t[PATIENT_ID] for t in meta])\n",
    "\n",
    "    train, test = train_test_split(df_id.drop_duplicates(), test_size=test_size, random_state=random_state)\n",
    "    indices_train = pd.merge(df_id.reset_index(), train, on=0)[\"index\"]\n",
    "    indices_test = pd.merge(df_id.reset_index(), test, on=0)[\"index\"]\n",
    "\n",
    "    x_train = [x[i] for i in indices_train]\n",
    "    x_test = [x[i] for i in indices_test]\n",
    "\n",
    "    y_train = [y[i] for i in indices_train]\n",
    "    y_test = [y[i] for i in indices_test]\n",
    "\n",
    "    meta_train = [meta[i] for i in indices_train]\n",
    "    meta_test = [meta[i] for i in indices_test]\n",
    "\n",
    "    return x_train, x_test, y_train, y_test, meta_train, meta_test"
   ],
   "id": "15206c8b9ddd4e09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x_train_pos, x_test_pos, y_train_pos, y_test_pos, meta_train_pos, meta_test_pos = split_data(x_pos, y_pos, meta_pos, random_state=42)\n",
    "x_train_neg, x_test_neg, y_train_neg, y_test_neg, meta_train_neg, meta_test_neg = split_data(x_neg, y_neg, meta_neg, random_state=42)\n",
    "\n",
    "bio_data_train, labels_train, meta_train = x_train_pos + x_train_neg, y_train_pos + y_train_neg, meta_train_pos + meta_train_neg\n",
    "bio_data_test, labels_test, meta_test = x_test_pos + x_test_neg, y_test_pos + y_test_neg, meta_test_pos + meta_test_neg"
   ],
   "id": "a1907bbb4866eaa0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_demographics(analysis_metadata, patients_table):\n",
    "    \"\"\"Given the metadata of an analysis (subject_id and storetime), computes the age of the corresponding patient at the time of the analysis.\n",
    "    Does not account for exact birthdate, but a +/-1 year difference is accepted\"\"\"\n",
    "    # FIXME we assume the id is effectively in the patients table \n",
    "    row = patients_table[patients_table[PATIENT_ID] == analysis_metadata[PATIENT_ID]].iloc[0][[\"gender\", \"anchor_age\", \"anchor_year\"]]\n",
    "    current_age = (\n",
    "            row[\"anchor_age\"] +  # age used as reference\n",
    "            pd.to_datetime(analysis_metadata[ANALYSIS_TIME]).year -  # year when the analysis was performed\n",
    "            pd.to_datetime(row[\"anchor_year\"], format=\"%Y\").year  # year used as reference\n",
    "    )\n",
    "    return {\"gender\":row[\"gender\"],\"age\": current_age}"
   ],
   "id": "152d06142e09a701",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tests with two data sources: a single biological analysis (creatinine) and demographics (age and gender).",
   "id": "f57efcfe454ca460"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# DATA EXTRACTION\n",
    "data_source_1 = [pd.DataFrame(t[\"analysis_50912\"]) for t in bio_data_train] # magnesium measurements, analysis with the least amount of missing data\n",
    "data_source_2 = pd.DataFrame([get_demographics(t, patients) for t in meta_train])\n",
    "labels = labels_train"
   ],
   "id": "a35ac6b89fb94e14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# FEATURE EXTRACTION\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def make_simple_imputer(strategy=\"mean\"):\n",
    "    return SimpleImputer(keep_empty_features=True, strategy=strategy)\n",
    "\n",
    "\n",
    "def impute_data(x, imputer):\n",
    "    \"\"\"\n",
    "    Performs data imputation on a per-matrix basis.\n",
    "    :param x: list of history matrices, as returned by `create_history_matrices`\n",
    "    :param imputer: the imputer to use to replace missing data\n",
    "    :return: a list of history matrices with imputed data\n",
    "    \"\"\"\n",
    "    data_x_imputed = []\n",
    "    for matrix in tqdm(x):\n",
    "        matrix_imputed = imputer.fit_transform(matrix)\n",
    "        data_x_imputed.append(matrix_imputed)\n",
    "    return data_x_imputed\n",
    "\n",
    "\n",
    "# Retrieves a pre-defined configuration file to extract all available features\n",
    "cfg = tsfel.get_features_by_domain()\n",
    "\n",
    "imputer = make_simple_imputer()\n",
    "x_train_imputed = impute_data(data_source_1, imputer)"
   ],
   "id": "7232f8668d57f636",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x_train_imputed[0]",
   "id": "bf407257f3705aeb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract features\n",
    "ds1_fe = tsfel.time_series_features_extractor(cfg, x_train_imputed).astype(float)"
   ],
   "id": "334a70a25353f81e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ds1_fe",
   "id": "3e788bdd6a479aa8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_source_2.head()",
   "id": "c24dd5dfcc4b94ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(drop=\"if_binary\")\n",
    "encoder.fit(data_source_2.loc[:, [\"gender\"]]) # using this syntax to keep the column as a DataFrame (necessary for one-hot encoding)\n",
    "ds2_gender_onehot = encoder.transform(data_source_2.loc[:, [\"gender\"]]).toarray()\n",
    "ds2_fe = pd.DataFrame(ds2_gender_onehot, columns=encoder.get_feature_names_out())\n",
    "ds2_fe[\"age\"] = data_source_2[\"age\"].astype(\"Int64\")"
   ],
   "id": "2134b285d2481dbb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ds2_fe.head()",
   "id": "f85a4eea100bb919",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ds1_fe.head()",
   "id": "7c783b90bd8f6db3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# if index are not aligned .concat() generates NaNs\n",
    "ds1_fe.reset_index(drop=True, inplace=True)\n",
    "ds2_fe.reset_index(drop=True, inplace=True)\n",
    "\n",
    "aggregation = pd.concat([ds1_fe, ds2_fe], axis=1) \n",
    "aggregation.head()"
   ],
   "id": "9d4b43d69c004ed5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "aggregation.shape",
   "id": "116ca0ac5dca4971",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "imputer = make_simple_imputer()\n",
    "x_train_imputed = imputer.fit_transform(aggregation)\n",
    "\n",
    "\n",
    "def scale_data(train_data, scaler):\n",
    "    x_train_shape = np.array(train_data).shape\n",
    "    num_analyses = x_train_shape[-1]\n",
    "\n",
    "    print(f\"Shape before: {x_train_shape}\")\n",
    "    print(f\"Number of analyses: {num_analyses}\")\n",
    "\n",
    "    print(\"Reshaping x_train and x_test...\")\n",
    "    x_train_reshape = np.stack(train_data).reshape(-1, num_analyses)\n",
    "\n",
    "    print(x_train_reshape.shape)\n",
    "    print(\"Fitting the scaler...\")\n",
    "    scaler.fit(x_train_reshape)\n",
    "\n",
    "    print(\"Scaling values...\")\n",
    "    x_train_reshape = scaler.transform(x_train_reshape)\n",
    "\n",
    "    print(\"Reshaping x_train and x_test...\")\n",
    "    x_train = x_train_reshape.reshape(x_train_shape)\n",
    "\n",
    "    print(\"Done!\")\n",
    "    print(f\"Shape after: {x_train.shape}\")\n",
    "\n",
    "    return x_train\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "x_train_scaled = scale_data(x_train_imputed, standard_scaler)\n",
    "\n",
    "ros = RandomOverSampler(random_state=25, sampling_strategy=0.5)\n",
    "x_train, y_train = ros.fit_resample(x_train_scaled, labels_train)\n",
    "y_train = np.array(y_train)"
   ],
   "id": "2ab94eba26c86e0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from keras.src.layers import Conv2D, BatchNormalization, Flatten, Activation, Dense, Conv1D\n",
    "from keras import Sequential\n",
    "\n",
    "num_rows = x_train.shape[0]\n",
    "num_columns = x_train.shape[1]\n",
    "n_filters = 3\n",
    "\n",
    "\n",
    "def test_model(rows, columns, output_dim, n_filters):\n",
    "    cnn = Sequential()\n",
    "    cnn.add(Conv1D(filters=n_filters, kernel_size=3, input_shape=(142,1)))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(Activation(\"relu\"))\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dense(32, activation='relu'))\n",
    "    cnn.add(Dense(output_dim, activation='sigmoid'))  # Output layer (adjust for your specific task)\n",
    "\n",
    "    print(\"Compiling the model...\")\n",
    "    cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    print(\"Model compiled!\")\n",
    "    return cnn\n",
    "\n",
    "model = test_model(\n",
    "    rows=num_rows,\n",
    "    columns=num_columns,\n",
    "    output_dim=1,  # binary output\n",
    "    n_filters=n_filters,\n",
    ")\n",
    "\n",
    "model.summary()"
   ],
   "id": "45948ec784bb8280",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.fit(x_train, y_train, epochs=3, batch_size=64)",
   "id": "fb3c9f332defe69b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(model, x_test, y_test, threshold=0.5):\n",
    "    _, test_accuracy = model.evaluate(x_test, y_test)\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_binary = pd.DataFrame(y_pred).apply(lambda val: (val > threshold).astype(int))\n",
    "\n",
    "    # Metrics\n",
    "    print(\"Test Accuracy: {:.2f}%\".format(test_accuracy * 100))\n",
    "    print(\"ROC-AUC: {:.2f}%\".format(roc_auc_score(y_test, y_pred_binary) * 100))\n",
    "    print(classification_report(y_test, y_pred_binary))\n",
    "\n",
    "    # Confusion matrix\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(121)\n",
    "    sns.heatmap(confusion_matrix(y_test, y_pred_binary, normalize=\"true\"), annot=True)\n",
    "    plt.subplot(122)\n",
    "    sns.heatmap(confusion_matrix(y_test, y_pred_binary, normalize=None), annot=True)\n",
    "    plt.show()\n",
    "    print(confusion_matrix(y_test, y_pred_binary, normalize=\"true\"))"
   ],
   "id": "62a2528a7fa26472",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "evaluate_model(model=model, x_test=x_train, y_test=y_train)",
   "id": "28d8620117a39133",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "meta_train[0]",
   "id": "9e187a226e14c30c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d4cf796ad4b02d3f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
